{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "zxq-BdGw61iP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install pandas numpy nltk Pillow requests tensorflow"
      ],
      "metadata": {
        "id": "qJwID_Gh_L6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "import json\n",
        "from io import BytesIO\n",
        "from PIL import Image, ImageEnhance\n",
        "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import math\n",
        "import pickle\n",
        "from collections import defaultdict\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# Ensure necessary NLTK datasets are downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "id": "Oc5mO3lU-JdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a69c364c-8546-4d62-ed9d-4ae308508eb0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1miCIbs-6vNd"
      },
      "outputs": [],
      "source": [
        "# Initialize the ResNet model globally\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False)\n",
        "model = Model(inputs=resnet_model.input, outputs=resnet_model.output)\n",
        "\n",
        "def preprocess_and_extract_features(image_url):\n",
        "    \"\"\"Preprocess an image from a URL and extract features using ResNet50.\"\"\"\n",
        "    try:\n",
        "        response = requests.get(image_url)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        image = image.resize((224, 224))\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(1.5)\n",
        "        enhancer = ImageEnhance.Brightness(image)\n",
        "        image = enhancer.enhance(1.1)\n",
        "        image_array = np.array(image)\n",
        "        image_array = preprocess_input(image_array)\n",
        "        image_array = np.expand_dims(image_array, axis=0)\n",
        "        features = model.predict(image_array)\n",
        "        return (image_url, features.reshape(features.shape[0], -1).tolist())\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image from URL {image_url}: {e}\")\n",
        "        return (image_url, None)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text data.\"\"\"\n",
        "    text = str(text).lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "def compute_tf(text):\n",
        "    \"\"\"Compute term frequency for a document.\"\"\"\n",
        "    tf_text = defaultdict(int)\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        tf_text[word] += 1\n",
        "    return {word: count / len(words) for word, count in tf_text.items()}\n",
        "\n",
        "def compute_idf(documents):\n",
        "    \"\"\"Compute inverse document frequency across a corpus.\"\"\"\n",
        "    idf_dict = defaultdict(int)\n",
        "    N = len(documents)\n",
        "    for document in documents:\n",
        "        unique_words = set(document.split())\n",
        "        for word in unique_words:\n",
        "            idf_dict[word] += 1\n",
        "    return {word: math.log(N / (count + 1)) for word, count in idf_dict.items()}\n",
        "\n",
        "def compute_tfidf(documents):\n",
        "    \"\"\"Compute TF-IDF scores for all documents in a corpus.\"\"\"\n",
        "    idf_dict = compute_idf(documents)\n",
        "    return [{word: tf * idf_dict[word] for word, tf in compute_tf(document).items()} for document in documents]\n",
        "\n",
        "def save_data(data, file_name):\n",
        "    \"\"\"Serialize data to a file.\"\"\"\n",
        "    with open(file_name, 'wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example DataFrame loading and processing\n",
        "    df = pd.read_csv('/content/drive/MyDrive/IR_Assignments/Assignment_2/A2_Data.csv')\n",
        "    df['Image'] = df['Image'].apply(lambda x: json.loads(x.replace(\"'\", \"\\\"\"))[0] if x else None)\n",
        "\n",
        "    # Process images and text\n",
        "    image_features_with_url = [preprocess_and_extract_features(url) for url in df['Image']]\n",
        "\n",
        "\n",
        "    # Save the results\n",
        "    save_data(image_features_with_url, '/content/drive/MyDrive/IR_Assignments/Assignment_2/image_features.pkl')\n",
        "\n",
        "    composite_data = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        image_url = row['Image']\n",
        "        review_text = row['Review Text']\n",
        "\n",
        "        # Process image and text\n",
        "        image_features = preprocess_and_extract_features(image_url)\n",
        "        preprocessed_review = preprocess_text(review_text)\n",
        "        tfidf_scores = compute_tfidf([preprocessed_review])[0]\n",
        "\n",
        "        composite_data.append({\n",
        "            'image_url': image_url,\n",
        "            'preprocessed_review': preprocessed_review,\n",
        "            'tfidf_score': tfidf_scores\n",
        "        })\n",
        "\n",
        "    save_data(composite_data, '/content/drive/MyDrive/IR_Assignments/Assignment_2/composite_data.pkl')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "def load_data(file_name):\n",
        "    \"\"\"Load serialized data from a file.\"\"\"\n",
        "    with open(file_name, 'rb') as file:\n",
        "        return pickle.load(file)\n",
        "\n",
        "# Specify the paths to your files\n",
        "image_features_path = '/content/drive/MyDrive/IR_Assignments/Assignment_2/image_features.pkl'\n",
        "processed_reviews_path = '/content/drive/MyDrive/IR_Assignments/Assignment_2/processed_reviews.pkl'\n",
        "tfidf_scores_path = '/content/drive/MyDrive/IR_Assignments/Assignment_2/tfidf_scores.pkl'\n",
        "\n",
        "\n",
        "# Load the data\n",
        "image_features = load_data(image_features_path)\n",
        "preprocessed_reviews = load_data(processed_reviews_path)\n",
        "tfidf_scores = load_data(tfidf_scores_path)\n",
        "\n",
        "# Print the contents\n",
        "# Note: For brevity and clarity, this example only prints the first few items.\n",
        "\n",
        "print(\"Image Features (first item):\", image_features[0][:10])  # Print first 10 features of the first item for brevity\n",
        "print(\"\\nProcessed Reviews (first 5):\", preprocessed_reviews[:5])\n",
        "print(\"\\nTF-IDF Scores (first document):\", list(tfidf_scores[0].items())[:])  # Print first 5 TF-IDF scores of the first document\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vpO_Q2V_Oix3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize ResNet model globally to avoid reloading\n",
        "resnet_model = ResNet50(weights='imagenet', include_top=False)\n",
        "model = Model(inputs=resnet_model.input, outputs=resnet_model.output)\n",
        "\n",
        "def preprocess_image(image_url):\n",
        "    \"\"\"Preprocess an image from a URL.\"\"\"\n",
        "    try:\n",
        "        image_url = str(image_url).strip().strip(\"'\").strip('\"')\n",
        "        response = requests.get(image_url)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "        image = image.resize((224, 224))\n",
        "        enhancer = ImageEnhance.Contrast(image)\n",
        "        image = enhancer.enhance(1.5)\n",
        "        enhancer = ImageEnhance.Brightness(image)\n",
        "        image = enhancer.enhance(1.1)\n",
        "        return np.array(image)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing image from URL {image_url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_features(image_array):\n",
        "    \"\"\"Extract features from an image array using ResNet50.\"\"\"\n",
        "    if image_array is not None:\n",
        "        image_array = preprocess_input(image_array)\n",
        "        image_array = np.expand_dims(image_array, axis=0)\n",
        "        features = model.predict(image_array)\n",
        "        return features.reshape(features.shape[0], -1).tolist()\n",
        "    return None\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"Preprocess text data.\"\"\"\n",
        "    text = str(text).lower().translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens if token not in stop_words]\n",
        "    return ' '.join(lemmatized_tokens)\n",
        "\n",
        "def compute_tf(text):\n",
        "    \"\"\"Compute term frequency for a document.\"\"\"\n",
        "    tf_text = defaultdict(int)\n",
        "    words = text.split()\n",
        "    for word in words:\n",
        "        tf_text[word] += 1\n",
        "    return {word: count / len(words) for word, count in tf_text.items()}\n",
        "\n",
        "def compute_idf(documents):\n",
        "    \"\"\"Compute inverse document frequency across a corpus.\"\"\"\n",
        "    idf_dict = defaultdict(int)\n",
        "    N = len(documents)\n",
        "    for document in documents:\n",
        "        unique_words = set(document.split())\n",
        "        for word in unique_words:\n",
        "            idf_dict[word] += 1\n",
        "    return {word: math.log(N / (count + 1)) for word, count in idf_dict.items()}\n",
        "\n",
        "def compute_tfidf(documents):\n",
        "    \"\"\"Compute TF-IDF scores for all documents in a corpus.\"\"\"\n",
        "    idf_dict = compute_idf(documents)\n",
        "    return [{word: tf * idf_dict[word] for word, tf in compute_tf(document).items()} for document in documents]\n",
        "\n",
        "# Cosine Similarity for numpy arrays (image features)\n",
        "def cosine_similarity(vector_a, vector_b):\n",
        "    # Ensure vectors are 1D\n",
        "    vector_a = vector_a.flatten()\n",
        "    vector_b = vector_b.flatten()\n",
        "    dot_product = np.dot(vector_a, vector_b)\n",
        "    norm_a = np.linalg.norm(vector_a)\n",
        "    norm_b = np.linalg.norm(vector_b)\n",
        "    return dot_product / (norm_a * norm_b) if (norm_a * norm_b) != 0 else 0\n",
        "\n",
        "\n",
        "\n",
        "# Cosine Similarity for dictionaries (TF-IDF vectors)\n",
        "def cosine_similarity_text(vec_a, vec_b):\n",
        "    # Ensure we're working with dense vectors if they're passed as sparse\n",
        "    if isinstance(vec_a, dict) and isinstance(vec_b, dict):\n",
        "        intersection = set(vec_a.keys()) & set(vec_b.keys())\n",
        "        numerator = sum([vec_a[x] * vec_b[x] for x in intersection])\n",
        "\n",
        "        sum1 = sum([val**2 for val in vec_a.values()])\n",
        "        sum2 = sum([float(val)**2 for val in vec_b.values()])  # Convert string values to float\n",
        "        denominator = np.sqrt(sum1) * np.sqrt(sum2)\n",
        "\n",
        "        return float(numerator) / denominator if denominator != 0 else 0.0\n",
        "    else:\n",
        "        raise ValueError(\"TF-IDF vectors must be of type dict.\")\n",
        "\n",
        "\n",
        "\n",
        "# Find Similar Images\n",
        "def find_most_similar(features_with_url, input_features, top_k=3):\n",
        "    similarities = []\n",
        "    for url, features in features_with_url:\n",
        "        if features is not None:\n",
        "            sim = cosine_similarity(input_features, np.array(features))\n",
        "            similarities.append((url, sim))\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_k]\n",
        "\n",
        "# Find Similar Reviews\n",
        "def find_most_similar_reviews(composite_data, input_tfidf, top_k=3):\n",
        "    \"\"\"Find and rank reviews based on text similarity.\"\"\"\n",
        "    similarities = []\n",
        "    for item in composite_data:\n",
        "        sim = cosine_similarity_text(input_tfidf, item['tfidf_score'])\n",
        "        similarities.append((item['image_url'], sim, item['preprocessed_review']))\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:top_k]\n",
        "\n",
        "def save_results(filename, data):\n",
        "    \"\"\"Save data to a pickle file.\"\"\"\n",
        "    with open(filename, 'wb') as file:\n",
        "        pickle.dump(data, file)\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Prompt user for input\n",
        "    image_url = input(\"Enter the image URL: \")\n",
        "    review_text = input(\"Enter the review text: \")\n",
        "\n",
        "    # Load pre-computed data\n",
        "    with open('/content/drive/MyDrive/IR_Assignments/Assignment_2/image_features.pkl', 'rb') as file:\n",
        "        image_features_with_url = pickle.load(file)\n",
        "    with open('//content/drive/MyDrive/IR_Assignments/Assignment_2/composite_data.pkl', 'rb') as file:\n",
        "        composite_data = pickle.load(file)\n",
        "\n",
        "    # Preprocess and compute features and TF-IDF for input\n",
        "    processed_image = preprocess_image(image_url)\n",
        "    input_image_features = extract_features(processed_image) if processed_image is not None else None\n",
        "    input_review_tfidf = compute_tfidf([preprocess_text(review_text)])[0]\n",
        "\n",
        "    # Find the top 3 similar images based on image features\n",
        "    similar_images = find_most_similar(image_features_with_url, np.array(input_image_features), top_k=3)\n",
        "\n",
        "    # Saving into pickled file\n",
        "    save_results('/content/drive/MyDrive/IR_Assignments/Assignment_2/similar_images.pkl', similar_images)\n",
        "\n",
        "    print(\"USING IMAGE RETRIEVAL\")\n",
        "    for idx, (img_url, img_sim) in enumerate(similar_images[:3], start=1):\n",
        "        # Find corresponding review in composite data and calculate text similarity\n",
        "        review_data = next((item for item in composite_data if item['image_url'] == img_url), None)\n",
        "        review_text = review_data['preprocessed_review'] if review_data else \"Review not found\"\n",
        "        text_sim = cosine_similarity_text(input_review_tfidf, review_data['tfidf_score']) if review_data else 0\n",
        "\n",
        "        print(f\"{idx}) Image URL: {img_url}\\nReview: {review_text}\\nCosine similarity of images - {img_sim:.3f}\\nCosine similarity of text - {text_sim:.4f}\\n\")\n",
        "\n",
        "    # Calculate and print composite scores for image retrieval\n",
        "    composite_image_similarity = sum([sim for _, sim in similar_images]) / len(similar_images)\n",
        "    composite_text_similarity = sum([text_sim for _, text_sim in similar_images]) / len(similar_images)\n",
        "    final_composite_similarity = (composite_image_similarity + composite_text_similarity) / 2\n",
        "    print(f\"\\nComposite similarity scores of images: {composite_image_similarity:.3f}\")\n",
        "    print(f\"Composite similarity scores of text: {composite_text_similarity:.3f}\")\n",
        "    print(f\"Final composite similarity score: {final_composite_similarity:.3f}\\n\")\n",
        "\n",
        "\n",
        "    # Find the top 3 similar reviews based on text similarity\n",
        "    similar_reviews = find_most_similar_reviews(composite_data, input_review_tfidf, top_k=3)\n",
        "\n",
        "    # Saving into pickled file\n",
        "    save_results('/content/drive/MyDrive/IR_Assignments/Assignment_2/similar_reviews.pkl', similar_reviews)\n",
        "\n",
        "\n",
        "    print(\"USING TEXT RETRIEVAL\")\n",
        "    for idx, (img_url, text_sim, review) in enumerate(similar_reviews, start=1):\n",
        "        # Find corresponding image feature similarity\n",
        "        img_features = next((features for url, features in image_features_with_url if url == img_url), None)\n",
        "        if img_features is not None:\n",
        "            img_sim = cosine_similarity(np.array(input_image_features), np.array(img_features))\n",
        "        else:\n",
        "            img_sim = 0  # In case there are no features found\n",
        "\n",
        "        print(f\"{idx}) Image URL: {img_url}\\nReview: {review}\\nCosine similarity of text - {text_sim:.3f}\\nCosine similarity of images - {img_sim:.3f}\\n\")\n",
        "\n",
        "    # Calculate and print composite scores for text retrieval\n",
        "    composite_image_similarity = sum([img_sim for _, img_sim, _ in similar_reviews]) / len(similar_reviews)\n",
        "    composite_text_similarity = sum([sim for _, sim, _ in similar_reviews]) / len(similar_reviews)\n",
        "    final_composite_similarity = (composite_image_similarity + composite_text_similarity) / 2\n",
        "    print(f\"\\nComposite similarity scores of images: {composite_image_similarity:.3f}\")\n",
        "    print(f\"Composite similarity scores of text: {composite_text_similarity:.3f}\")\n",
        "    print(f\"Final composite similarity score: {final_composite_similarity:.3f}\\n\")\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\n"
      ],
      "metadata": {
        "id": "XrILEjtDHjSe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50d84402-f852-408b-e9a8-e1e38f8edb6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 1s 0us/step\n",
            "Enter the image URL: https://images-na.ssl-images-amazon.com/images/I/712LuK8v2+L._SY88.jpg\n",
            "Enter the review text: Loving\n",
            "1/1 [==============================] - 1s 1s/step\n",
            "USING IMAGE RETRIEVAL\n",
            "1) Image URL: https://images-na.ssl-images-amazon.com/images/I/712LuK8v2+L._SY88.jpg\n",
            "Review: hot damn replaced original neck position humbucker 1980 le paul bought new sph90 phat cat difference much louder brighter around sweeter tuneful original used rarely use neck pickup alone ill use almost exclusively though mixing bit bridge humbucker sweet im glad bought made switch p ive problem hum pickup problem probably somewhere amp\n",
            "Cosine similarity of images - 1.000\n",
            "Cosine similarity of text - 0.0000\n",
            "\n",
            "2) Image URL: https://images-na.ssl-images-amazon.com/images/I/71Y9QQZDpvL._SY88.jpg\n",
            "Review: love great finish quality\n",
            "Cosine similarity of images - 0.310\n",
            "Cosine similarity of text - 0.0000\n",
            "\n",
            "3) Image URL: https://images-na.ssl-images-amazon.com/images/I/71Bj0KzjFtL._SY88.jpg\n",
            "Review: nice pickguard fit perfectly\n",
            "Cosine similarity of images - 0.310\n",
            "Cosine similarity of text - 0.0000\n",
            "\n",
            "\n",
            "Composite similarity scores of images: 0.540\n",
            "Composite similarity scores of text: 0.540\n",
            "Final composite similarity score: 0.540\n",
            "\n",
            "USING TEXT RETRIEVAL\n",
            "1) Image URL: https://images-na.ssl-images-amazon.com/images/I/71YEX7X28kL._SY88.jpg\n",
            "Review: say im loving\n",
            "Cosine similarity of text - 0.577\n",
            "Cosine similarity of images - 0.114\n",
            "\n",
            "2) Image URL: https://images-na.ssl-images-amazon.com/images/I/81eTaHUrNeL._SY88.jpg\n",
            "Review: great little compact mixer super versatile loving basement studio\n",
            "Cosine similarity of text - 0.333\n",
            "Cosine similarity of images - 0.174\n",
            "\n",
            "3) Image URL: https://images-na.ssl-images-amazon.com/images/I/81q5+IxFVUL._SY88.jpg\n",
            "Review: loving vintage spring vintage strat good tension great stability floating bridge want spring way go\n",
            "Cosine similarity of text - 0.229\n",
            "Cosine similarity of images - 0.307\n",
            "\n",
            "\n",
            "Composite similarity scores of images: 0.380\n",
            "Composite similarity scores of text: 0.380\n",
            "Final composite similarity score: 0.380\n",
            "\n"
          ]
        }
      ]
    }
  ]
}